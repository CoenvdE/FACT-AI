{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IW_naPEc_-SO"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vw2bsOjuIA4X"
      },
      "outputs": [],
      "source": [
        "# Installations (uncomment if needed)\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install datasets\n",
        "!pip install captum\n",
        "!pip install tqdm\n",
        "!pip install torchcam\n",
        "\n",
        "# System and OS\n",
        "import os\n",
        "import os.path as osp\n",
        "from collections import OrderedDict\n",
        "\n",
        "# Basic Libraries\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "\n",
        "# PyTorch related\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "# CLIP and related libraries\n",
        "from clip import clip\n",
        "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
        "\n",
        "# Model and Data Processing\n",
        "from transformers import AlignTextModel, AlignProcessor, AlignModel\n",
        "from PIL import Image\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from captum.attr import Saliency\n",
        "from captum.attr import visualization as viz\n",
        "\n",
        "# CAM methods\n",
        "from torchcam.methods import GradCAM\n",
        "from torchcam.utils import overlay_mask\n",
        "\n",
        "# Datasets\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCBUyJUPMWYT"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jX41uM2Y_064"
      },
      "outputs": [],
      "source": [
        "# LOAD DATALOADERS WITH TRANSFORMED IMAGES\n",
        "\n",
        "\n",
        "#using the ImageNet Transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224), # Cropping a central square patch of the image\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "access_token = INSERT OWN HUGGINGFACE TOKEN HERE
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for item in batch:\n",
        "        image = item['image']\n",
        "        label = item['label']\n",
        "\n",
        "        # Convert to PIL Image if not already (assuming image is a NumPy array or a tensor)\n",
        "        if not isinstance(image, Image.Image):\n",
        "            image = to_pil_image(image)\n",
        "\n",
        "        # Ensure the image is in RGB format\n",
        "        if image.mode != 'RGB':\n",
        "            image = image.convert('RGB')\n",
        "\n",
        "        # Apply transformations\n",
        "        image = transform(image)\n",
        "\n",
        "        # Append the transformed image and label to the lists\n",
        "        images.append(image)\n",
        "        labels.append(label)\n",
        "\n",
        "    # Stack images into a single tensor and convert labels to tensor\n",
        "    images = torch.stack(images)\n",
        "    labels = torch.tensor(labels)\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "subset_size = 10000  # Adjust this based on your needs, 10.000 is almost too much\n",
        "subset_data = []\n",
        "imagenet_data = load_dataset(\"imagenet-1k\", split=\"train\", streaming = True, token=access_token, trust_remote_code=True)\n",
        "\n",
        "# Manually iterate through the dataset and take a subset\n",
        "for i, sample in enumerate(imagenet_data):\n",
        "    if i >= subset_size:\n",
        "        break\n",
        "    subset_data.append(sample)\n",
        "\n",
        "# creating a DataLoader from this subset\n",
        "dataloader = DataLoader(subset_data, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "# Get validation set\n",
        "subset_size = 1000  # Adjust this based on your needs, 10.000 is almost too much\n",
        "subset_data = []\n",
        "validation_data = load_dataset(\"imagenet-1k\", split=\"validation\", streaming = True, token=access_token, trust_remote_code=True)\n",
        "for i, sample in enumerate(validation_data):\n",
        "    if i >= subset_size:\n",
        "        break\n",
        "    subset_data.append(sample)\n",
        "validation_loader = DataLoader(subset_data, batch_size=24, shuffle=None, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SArU2BJ3AHjH"
      },
      "outputs": [],
      "source": [
        "def get_ImageNet_ClassNames():\n",
        "    \"\"\"\n",
        "    Reads and returns a list of class names from the ImageNet dataset.\n",
        "\n",
        "    This function reads a JSON file containing mappings of ImageNet class indices\n",
        "    to their respective human-readable names and returns a list of these names.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of strings where each string is a class name from ImageNet.\n",
        "    \"\"\"\n",
        "    # Path to the JSON file containing ImageNet class index and names\n",
        "    text_file = '/content/drive/MyDrive/FACT LICO 13/imagenet_class_index.json'\n",
        "\n",
        "    # Open the JSON file and load its contents into a Python dictionary\n",
        "    with open(text_file, 'r', encoding='utf-8') as f:\n",
        "        class_index = json.load(f)\n",
        "\n",
        "    # Initialize an empty list to hold the class names\n",
        "    names = []\n",
        "\n",
        "    # Iterate over the dictionary and extract class names\n",
        "    for i in range(len(class_index)):\n",
        "        # Append the last element (class name) of each list in the dictionary to 'names'\n",
        "        name = class_index[str(i)].replace(\"_\", \" \")\n",
        "\n",
        "        names.append(name)\n",
        "\n",
        "    # Return the list of class names\n",
        "    return names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysw9xtyl3IHl"
      },
      "outputs": [],
      "source": [
        "def get_encoded_labels(labels, prompt):\n",
        "    \"\"\"\n",
        "    Get prompts that correspond with labels of given batch.\n",
        "    \"\"\"\n",
        "    labels = labels.to(torch.int64)\n",
        "    selected_encodings = prompt[labels]\n",
        "    return selected_encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7_ztLUw_4vw"
      },
      "outputs": [],
      "source": [
        "class ModifiedResNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Resnet50 model structure, with modification that it also returns the image features\n",
        "    before the fully connected layer. This is used for OT loss\n",
        "    \"\"\"\n",
        "    def __init__(self, original_model):\n",
        "        super(ModifiedResNet, self).__init__()\n",
        "\n",
        "        # add layers from the original model\n",
        "        for name, module in original_model.named_children():\n",
        "            setattr(self, name, module)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        features = x # [Batch_size, number of filters, feature_map_height, feature_map_width]\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        logits = self.fc(x)\n",
        "        return logits, features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLeoQ8FvMFku"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    MLP to transform text_features to image features dimensions to use text encoder\n",
        "    with any image encoder\n",
        "    the temperature parameter is trained here and used for MM loss in the training loop\n",
        "    output_dim is the dimension of the image_feature output of the image encoder\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        # Temperature as trainable param for mm loss\n",
        "        self.temp = nn.Parameter(torch.ones([]) * np.log(1 / 0.07)) # DAAN LETS CHECK VALUES, MAYBE TRESHOLD\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVLf2IYCT1dF"
      },
      "outputs": [],
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A text encoder module that uses a transformer model from a CLIP architecture\n",
        "    to encode text prompts into feature embeddings.\n",
        "\n",
        "    Attributes:\n",
        "        transformer (nn.Module): The transformer module from the CLIP model.\n",
        "        positional_embedding (Tensor): The positional embeddings from the CLIP model.\n",
        "        ln_final (nn.Module): Layer normalization applied after the transformer.\n",
        "        text_projection (Tensor): Linear projection layer for final text features.\n",
        "        dtype (torch.dtype): Data type of the model, typically torch.FloatTensor.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, clip_model):\n",
        "        \"\"\"\n",
        "        Initializes the TextEncoder module using components from a given CLIP model.\n",
        "\n",
        "        Args:\n",
        "            clip_model (CLIP): A pre-trained CLIP model.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Adjust the CLIP model to the appropriate data type (float)\n",
        "        clip_model = clip_model.type(torch.FloatTensor)\n",
        "\n",
        "        # Extract relevant parts from the CLIP model\n",
        "        self.transformer = clip_model.transformer\n",
        "        self.positional_embedding = clip_model.positional_embedding\n",
        "        self.ln_final = clip_model.ln_final\n",
        "        self.text_projection = clip_model.text_projection\n",
        "        self.dtype = clip_model.dtype\n",
        "\n",
        "    def forward(self, prompts, tokenized_prompts):\n",
        "        \"\"\"\n",
        "        Forward pass for encoding all text prompts.\n",
        "\n",
        "        Args:\n",
        "            prompts (Tensor): The context vectors for prompts of all classes.\n",
        "            tokenized_prompts (Tensor): Tokenized representation of the prompts of all classes.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The encoded text features.\n",
        "        \"\"\"\n",
        "        # Add positional embeddings to prompts and adjust dimensions for transformer\n",
        "        x = prompts + self.positional_embedding.type(self.dtype)\n",
        "        x = x.permute(1, 0, 2)  # Reorder dimensions for transformer input. # (batch, length, dimension) -> (length, batch, dimension) for transformer\n",
        "\n",
        "        # Pass the input through the transformer\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)  # Reorder dimensions back to original.  # (batch, length, dimension) <- (length, batch, dimension) for transformer\n",
        "\n",
        "        # Apply layer normalization\n",
        "        x = self.ln_final(x).type(self.dtype)\n",
        "\n",
        "        # Extract features corresponding to the end-of-token (EOT) embedding\n",
        "        # and apply text projection to get final text feature embeddings\n",
        "        # EOT: embeddings of entire input sequence\n",
        "        # self.text_projection is a learned linear transformation\n",
        "        # maps the high-dimensional transformer output to a lower-dimensional space suitable for downstream tasks\n",
        "        # print(f'x{x.shape}')\n",
        "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7vCN4DfT1h8"
      },
      "outputs": [],
      "source": [
        "# COMMENT AND CHECK LATER\n",
        "\n",
        "class PromptLearner(nn.Module):\n",
        "    \"\"\"\n",
        "    A PyTorch module for learning prompt embeddings in the context of a CLIP model.\n",
        "\n",
        "    This module creates and learns context vectors (prompts) for each class in a given set of class names.\n",
        "    These prompts are used with a CLIP model to produce text embeddings that are aligned with image features.\n",
        "\n",
        "    Attributes:\n",
        "        ctx (nn.Parameter): Learnable context vectors for each class.\n",
        "        token_prefix (Tensor): Start-of-sequence token embeddings from CLIP.\n",
        "        token_suffix (Tensor): End-of-sequence and class token embeddings from CLIP.\n",
        "        n_cls (int): Number of classes.\n",
        "        n_ctx (int): Number of context tokens.\n",
        "        class_token_position (str): Position of the class token in the prompt (options: 'middle', 'end', 'front').\n",
        "    \"\"\"\n",
        "    def __init__(self, classnames, clip_model):\n",
        "        \"\"\"\n",
        "        Initializes the PromptLearner module with class names and a CLIP model.\n",
        "\n",
        "        Args:\n",
        "            classnames (list): A list of class names (strings).\n",
        "            clip_model (CLIP): The pre-trained CLIP model from which certain layers are used.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        n_cls = len(classnames)\n",
        "        n_ctx = 12\n",
        "        ctx_init = None\n",
        "        dtype = clip_model.dtype\n",
        "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
        "        self.N = 1\n",
        "\n",
        "        if ctx_init:\n",
        "            # use given words to initialize context vectors\n",
        "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
        "            n_ctx = len(ctx_init.split(\" \"))\n",
        "            prompt = clip.tokenize(ctx_init)\n",
        "            with torch.no_grad():\n",
        "                embedding = clip_model.token_embedding(prompt).type(dtype)\n",
        "            ctx_vectors = embedding[0, 1 : 1 + n_ctx, :]\n",
        "            prompt_prefix = ctx_init\n",
        "        else:\n",
        "            # random initialization, DAAN: not random, right? we initialize with X's\n",
        "            if True:\n",
        "                print(\"Initializing class-specific contexts\")\n",
        "                ctx_vectors = torch.empty(n_cls, n_ctx, ctx_dim, dtype=dtype)\n",
        "            else:\n",
        "                print(\"Initializing a generic context\")\n",
        "                ctx_vectors = torch.empty(self.N, n_ctx, ctx_dim, dtype=dtype)\n",
        "            nn.init.normal_(ctx_vectors, std=0.02)   # define the prompt to be trained\n",
        "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
        "\n",
        "        print(f'Initial context: \"{prompt_prefix}\"')\n",
        "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
        "\n",
        "        self.ctx = nn.Parameter(ctx_vectors)  # to be optimized\n",
        "\n",
        "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
        "        name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
        "\n",
        "        # '.' as end of sentence token for representation of whole sentence\n",
        "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
        "\n",
        "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]) # (10, 77)\n",
        "        tokenized_prompts = tokenized_prompts.repeat(self.N,1)\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embedding = clip_model.token_embedding(tokenized_prompts).type(dtype)\n",
        "        print('tokenized prompts:', embedding.shape, 'ctx: ', self.ctx.shape)\n",
        "\n",
        "        # These token vectors will be saved when in save_model(),\n",
        "        # but they should be ignored in load_model() as we want to use\n",
        "        # those computed using the current class names DAAN: huh??? So we don't use it??\n",
        "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS\n",
        "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx :, :])  # CLS, EOS\n",
        "\n",
        "        self.n_cls = n_cls\n",
        "        self.n_ctx = n_ctx\n",
        "        self.tokenized_prompts = tokenized_prompts  # torch.Tensor\n",
        "        self.name_lens = name_lens\n",
        "        self.class_token_position = 'middle'\n",
        "\n",
        "    def _ctx_shuffle(self, prefix, suffix, ctx, cls_loc = 'end', shuffleCLS = False):\n",
        "        \"\"\"\n",
        "        Shuffles the context vectors.\n",
        "\n",
        "        Args:\n",
        "            prefix (Tensor): Prefix token embeddings.\n",
        "            suffix (Tensor): Suffix token embeddings.\n",
        "            ctx (Tensor): Context vectors to shuffle.\n",
        "            cls_loc (str): Position of the class token in the prompt.\n",
        "            shuffleCLS (bool): Whether to shuffle the class token positions.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Shuffled context vectors.\n",
        "        \"\"\"\n",
        "\n",
        "        # shuffle the ctx along 2nd dimension\n",
        "        rand_idx = torch.randperm(ctx.shape[1])\n",
        "        shuffled_ctx = ctx[:, rand_idx, :]\n",
        "        return shuffled_ctx\n",
        "\n",
        "\n",
        "    def forward(self):\n",
        "        \"\"\"\n",
        "        Forward pass of the PromptLearner to create prompts for each class.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: A batch of prompts, one for each class.\n",
        "        \"\"\"\n",
        "\n",
        "        ctx = self.ctx\n",
        "        if ctx.dim() == 3:\n",
        "            ctx = ctx.unsqueeze(0)\n",
        "\n",
        "        ctx = ctx.contiguous().view(self.N*self.n_cls,self.n_ctx,ctx.shape[3])\n",
        "\n",
        "        prefix = self.token_prefix\n",
        "        suffix = self.token_suffix\n",
        "\n",
        "        ctx = self._ctx_shuffle(prefix, suffix, ctx)\n",
        "\n",
        "        if self.class_token_position == \"end\":\n",
        "            prompts = torch.cat(\n",
        "                [\n",
        "                    prefix,  # (n_cls, 1, dim)\n",
        "                    ctx,     # (n_cls, n_ctx, dim)\n",
        "                    suffix,  # (n_cls, *, dim)\n",
        "                ],\n",
        "                dim=1,\n",
        "            )\n",
        "\n",
        "        elif self.class_token_position == \"middle\":\n",
        "            half_n_ctx = self.n_ctx // 2\n",
        "            prompts = []\n",
        "            for i in range(self.n_cls):\n",
        "                name_len = self.name_lens[i]\n",
        "                prefix_i = prefix[i : i + 1, :, :]\n",
        "                class_i = suffix[i : i + 1, :name_len, :]\n",
        "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
        "                ctx_i_half1 = ctx[i : i + 1, :half_n_ctx, :]\n",
        "                ctx_i_half2 = ctx[i : i + 1, half_n_ctx:, :]\n",
        "                prompt = torch.cat(\n",
        "                    [\n",
        "                        prefix_i,     # (1, 1, dim)\n",
        "                        ctx_i_half1,  # (1, n_ctx//2, dim)\n",
        "                        class_i,      # (1, name_len, dim)\n",
        "                        ctx_i_half2,  # (1, n_ctx//2, dim)\n",
        "                        suffix_i,     # (1, *, dim)\n",
        "                    ],\n",
        "                    dim=1,\n",
        "                )\n",
        "                prompts.append(prompt)\n",
        "            prompts = torch.cat(prompts, dim=0)\n",
        "\n",
        "        elif self.class_token_position == \"front\":\n",
        "            prompts = []\n",
        "            for i in range(self.n_cls):\n",
        "                name_len = self.name_lens[i]\n",
        "                prefix_i = prefix[i : i + 1, :, :]\n",
        "                class_i = suffix[i : i + 1, :name_len, :]\n",
        "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
        "                ctx_i = ctx[i : i + 1, :, :]\n",
        "                prompt = torch.cat(\n",
        "                    [\n",
        "                        prefix_i,  # (1, 1, dim)\n",
        "                        class_i,   # (1, name_len, dim)\n",
        "                        ctx_i,     # (1, n_ctx, dim)\n",
        "                        suffix_i,  # (1, *, dim)\n",
        "                    ],\n",
        "                    dim=1,\n",
        "                )\n",
        "                prompts.append(prompt)\n",
        "            prompts = torch.cat(prompts, dim=0)\n",
        "\n",
        "        else:\n",
        "            raise ValueError\n",
        "        return prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQFA5ysw4Kjc"
      },
      "outputs": [],
      "source": [
        "class_names = get_ImageNet_ClassNames()\n",
        "\n",
        "# Load CLIP model\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "\n",
        "# Tokenize and encode class names\n",
        "text_inputs = clip.tokenize(class_names).to(device)\n",
        "with torch.no_grad():\n",
        "    text_features_alt = model.encode_text(text_inputs).float()\n",
        "\n",
        "print(text_features_alt.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3at-qimtenr"
      },
      "outputs": [],
      "source": [
        "# CALCULATIONS FOR OT LOSS\n",
        "\n",
        "\n",
        "# Adapted from https://github.com/gpeyre/SinkhornAutoDiff\n",
        "class SinkhornDistance(nn.Module):\n",
        "    \"\"\"\n",
        "    Given two empirical measures each with :math:`P_1` locations\n",
        "    :math:`x\\in\\mathbb{R}^{D_1}` and :math:`P_2` locations :math:`y\\in\\mathbb{R}^{D_2}`,\n",
        "    outputs an approximation of the regularized OT cost for point clouds.\n",
        "    Args:\n",
        "        eps (float): regularization coefficient\n",
        "        max_iter (int): maximum number of Sinkhorn iterations\n",
        "        reduction (string, optional): Specifies the reduction to apply to the output:\n",
        "            'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n",
        "            'mean': the sum of the output will be divided by the number of\n",
        "            elements in the output, 'sum': the output will be summed. Default: 'none'\n",
        "    Shape:\n",
        "        - Input: :math:`(N, P_1, D_1)`, :math:`(N, P_2, D_2)`\n",
        "        - Output: :math:`(N)` or :math:`()`, depending on `reduction`\n",
        "    \"\"\"\n",
        "    def __init__(self, eps, max_iter, reduction='none'):\n",
        "        super(SinkhornDistance, self).__init__()\n",
        "        self.eps = eps\n",
        "        self.max_iter = max_iter\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # The Sinkhorn algorithm takes as input three variables :\n",
        "        C = self._cost_matrix(x, y)  # Wasserstein cost function\n",
        "        # print(x.size(), y.size(), C.shape)\n",
        "        x_points = x.shape[-2]\n",
        "        y_points = y.shape[-2]\n",
        "        # print(x.dim(), x_points, y_points)\n",
        "        if x.dim() == 2:\n",
        "            batch_size = 1\n",
        "        else:\n",
        "            batch_size = x.shape[0]\n",
        "\n",
        "        # both marginals are fixed with equal weights\n",
        "        mu = torch.empty(batch_size, x_points, dtype=torch.float,\n",
        "                         requires_grad=False).fill_(1.0 / x_points).squeeze().cuda()\n",
        "        nu = torch.empty(batch_size, y_points, dtype=torch.float,\n",
        "                         requires_grad=False).fill_(1.0 / y_points).squeeze().cuda()\n",
        "\n",
        "        u = torch.zeros_like(mu).cuda()\n",
        "        v = torch.zeros_like(nu).cuda()\n",
        "        # To check if algorithm terminates because of threshold\n",
        "        # or max iterations reached\n",
        "        actual_nits = 0\n",
        "        # Stopping criterion\n",
        "        thresh = 1e-3\n",
        "\n",
        "        # Sinkhorn iterations\n",
        "        for i in range(self.max_iter):\n",
        "            u1 = u  # useful to check the update\n",
        "            u = self.eps * (torch.log(mu+1e-8) - torch.logsumexp(self.M(C, u, v), dim=-1)) + u\n",
        "            v = self.eps * (torch.log(nu+1e-8) - torch.logsumexp(self.M(C, u, v).transpose(-2, -1), dim=-1)) + v\n",
        "            err = (u - u1).abs().sum(-1).mean()\n",
        "\n",
        "            actual_nits += 1\n",
        "            # print(i, err.item(), thresh)\n",
        "            if err.item() < thresh:\n",
        "                break\n",
        "\n",
        "        U, V = u, v\n",
        "        # Transport plan pi = diag(a)*K*diag(b)\n",
        "        pi = torch.exp(self.M(C, U, V))\n",
        "\n",
        "        # Sinkhorn distance\n",
        "        cost = torch.sum(pi * C, dim=(-2, -1)).mean()\n",
        "\n",
        "        # if self.reduction == 'mean':\n",
        "        #     cost = cost.mean()\n",
        "        # elif self.reduction == 'sum':\n",
        "        #     cost = cost.sum()\n",
        "\n",
        "        return cost\n",
        "\n",
        "    def M(self, C, u, v):\n",
        "        \"Modified cost for logarithmic updates\"\n",
        "        \"$M_{ij} = (-c_{ij} + u_i + v_j) / \\epsilon$\"\n",
        "        return (-C + u.unsqueeze(-1) + v.unsqueeze(-2)) / self.eps\n",
        "\n",
        "    @staticmethod\n",
        "    def _cost_matrix(x, y, p=2):\n",
        "        \"Returns the matrix of $|x_i-y_j|^p$.\"\n",
        "        # print(x.shape, y.shape)\n",
        "        x_col = x.unsqueeze(-2)\n",
        "        y_lin = y.unsqueeze(-3)\n",
        "        # print(x_col.shape, y_lin.shape)\n",
        "        C = torch.sum((torch.abs(x_col - y_lin)) ** p, -1)\n",
        "        # C.detach()\n",
        "        return C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXW4yyHnAdw-"
      },
      "outputs": [],
      "source": [
        "# MAKE FUNCTION OF IT\n",
        "def calculate_adjacency_matrix(features, temperature):\n",
        "    \"\"\"\n",
        "    calculates the adjacency matrix of the given features.\n",
        "    1. Calculate the pairwise Euclidean distances of the features\n",
        "    2. Apply the temperature scaling\n",
        "    \"\"\"\n",
        "    dist_matrix = torch.cdist(features, features, p=2)\n",
        "    adj_matrix = F.softmax(-dist_matrix / temperature, dim=1)\n",
        "    return adj_matrix\n",
        "\n",
        "\n",
        "def manifold_matching_loss(image_features, text_features, temperature):\n",
        "    \"\"\"\n",
        "    calculate the mm loss of the lico model\n",
        "    \"\"\"\n",
        "    A_F = calculate_adjacency_matrix(image_features, temperature)\n",
        "    A_G = calculate_adjacency_matrix(text_features, temperature)\n",
        "\n",
        "    # Calculate the KL divergence loss for manifold matching\n",
        "    loss = F.kl_div(A_G.log(), A_F, reduction='batchmean')\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06l0U8vthab7"
      },
      "outputs": [],
      "source": [
        "def train_model(modified_resnet, dataloader, manifold_matching_loss, sinkhorn_loss, text2img_dim_transform, num_epochs, device, all_prompt_features, validation_loader, get_encoded_labels, ablation1, ablation2):\n",
        "    \"\"\"\n",
        "    Train the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Check validation before training\n",
        "    validate_model(modified_resnet, validation_loader, device)\n",
        "\n",
        "    # initialize the optimizer\n",
        "    optimizer = optim.SGD([\n",
        "        {'params': modified_resnet.parameters()},\n",
        "        {'params': text2img_dim_transform.parameters()},\n",
        "    ], lr=0.03, momentum=0.9, weight_decay=0.0001)\n",
        "\n",
        "    # Initialize the learning rate scheduler\n",
        "    scheduler = CosineAnnealingLR(optimizer, num_epochs)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        modified_resnet.train()\n",
        "        text2img_dim_transform.train()\n",
        "\n",
        "\n",
        "        if (epoch+1) % 2 == 1: # Change this to show which epoch we are\n",
        "            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        for images, labels in tqdm(dataloader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass through models\n",
        "            encoded_labels = get_encoded_labels(labels, all_prompt_features)\n",
        "            predictions, features_resnet = modified_resnet(images)\n",
        "\n",
        "            # feature_maps for OT loss\n",
        "            feature_maps = features_resnet.view(features_resnet.shape[0], features_resnet.shape[1], -1)\n",
        "            feature_maps = F.normalize(feature_maps, dim = 2)\n",
        "\n",
        "            # image_features for manifold loss\n",
        "            image_features = F.adaptive_avg_pool2d(features_resnet, 1)\n",
        "            image_features = image_features.view(images.shape[0], -1)\n",
        "            image_features = F.normalize(image_features, dim = -1)\n",
        "\n",
        "            # transform text_features dimension to match thos of the image encoder's output\n",
        "            text_features = text2img_dim_transform(encoded_labels)\n",
        "            text_features = F.normalize(text_features, dim = -1)\n",
        "\n",
        "            # get temperature parameter\n",
        "            temperature = text2img_dim_transform.temp\n",
        "\n",
        "            # calculate losses\n",
        "            CE_loss = torch.nn.functional.cross_entropy(predictions, labels)\n",
        "            MM_loss = manifold_matching_loss(image_features, text_features, temperature)\n",
        "            OT_loss = sinkhorn_loss(feature_maps, text_features)\n",
        "\n",
        "            if ablation1 == 'mm' or ablation2 == 'mm':\n",
        "                MM_loss = 0\n",
        "            if ablation1 == 'ot' or ablation2 == 'ot':\n",
        "                OT_loss = 0\n",
        "\n",
        "            # params according to the paper\n",
        "            alpha = 10\n",
        "            beta = 1\n",
        "\n",
        "            # Combine the losses or use them as needed\n",
        "            total_loss = CE_loss + alpha * MM_loss + beta * OT_loss\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Clipping the parameter value to be within a min_val and max_val #CHOSEN BY OURSELVES\n",
        "            with torch.no_grad():  # This makes sure the operation is not tracked by autograd\n",
        "                text2img_dim_transform.temp.clamp_(min=0.1, max=3)\n",
        "\n",
        "\n",
        "        print(f\"temperature after last batch of epoch was:{temperature.item()}\")\n",
        "\n",
        "        # Evaluate on validation set or perform any other actions at the end of each epoch\n",
        "        validate_model(modified_resnet, validation_loader, device)\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"Loss of last epoch in batch is: CE: {CE_loss}, OT: {OT_loss}, MM: {MM_loss}\")\n",
        "\n",
        "        # Save the model after training\n",
        "        torch.save(modified_resnet.state_dict(), f'/content/drive/MyDrive/FACT LICO 13/Models/modified_resnet_{ablation1}{ablation2}_{epoch}.pth') # CHANGE PATH ALWAYS\n",
        "        torch.save(text2img_dim_transform.state_dict(), f'/content/drive/MyDrive/FACT LICO 13/Models/text2img_dim_transform_{ablation1}{ablation2}_{epoch}.pth') # CHANGE PATH ALWAYS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a11pP4N22eEG"
      },
      "outputs": [],
      "source": [
        "def validate_model(modified_resnet, dataloader, device):\n",
        "    \"\"\"\n",
        "    Validate the model.\n",
        "    \"\"\"\n",
        "\n",
        "    modified_resnet.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # Initialize variables to track metrics\n",
        "    total_accuracy = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    with torch.no_grad():  # No need to track gradients during validation\n",
        "        for images, labels in tqdm(dataloader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass through models\n",
        "            predictions, _ = modified_resnet(images)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(predictions.data, 1)\n",
        "            total_accuracy += (predicted == labels).sum().item()\n",
        "            num_batches += 1\n",
        "\n",
        "    # Compute average losses and accuracy\n",
        "    avg_accuracy = total_accuracy / (num_batches * dataloader.batch_size)\n",
        "\n",
        "    print(f'Validation results: Accuracy: {avg_accuracy}')\n",
        "\n",
        "    # Return to training mode\n",
        "    modified_resnet.train()\n",
        "    return avg_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDe0ypKnzBuA"
      },
      "outputs": [],
      "source": [
        "clip_model, _ = clip.load(\"ViT-B/32\", device)\n",
        "classnames = get_ImageNet_ClassNames()\n",
        "_tokenizer = _Tokenizer()\n",
        "\n",
        "# 1. For pronmptleaner\n",
        "text_encoder = TextEncoder(clip_model).to(device)\n",
        "prompt_learner = PromptLearner(classnames, clip_model).to(device)\n",
        "total_prompt_from_labels = prompt_learner()\n",
        "tokenized_total_prompt = prompt_learner.tokenized_prompts.to(device)\n",
        "with torch.no_grad():\n",
        "    all_prompt_features = text_encoder(total_prompt_from_labels, tokenized_total_prompt)\n",
        "\n",
        "# For without Promptlearner\n",
        "# all_prompt_features = text_features_alt\n",
        "\n",
        "all_prompt_features = all_prompt_features.to(device)\n",
        "\n",
        "# create the resnet model\n",
        "resnet = models.resnet50(pretrained=False)\n",
        "resnet = resnet.to(device)\n",
        "modified_resnet = ModifiedResNet(resnet)\n",
        "modified_resnet = modified_resnet.to(device)\n",
        "\n",
        "\n",
        "# SAVE UNTRAINED MODEL FOR LATER COMPARISON\n",
        "pre_training_weights = copy.deepcopy(modified_resnet.state_dict())\n",
        "\n",
        "# create mlp\n",
        "input_dim = 512 # text encoder CLIP\n",
        "output_dim = 49 # to match 7x7 dimension of the feature maps\n",
        "hidden_dim = 512 # COEN: chat said the notation is hidden dim, output dim, so hidden dim = 512\n",
        "text2img_dim_transform = MLP(input_dim, hidden_dim, output_dim)\n",
        "text2img_dim_transform = text2img_dim_transform.to(device)\n",
        "\n",
        "\n",
        "# Initialize SinkhornDistance module\n",
        "sinkhorn_loss = SinkhornDistance(eps=0.1, max_iter=100, reduction='mean').to(device)\n",
        "\n",
        "# train model\n",
        "num_epochs = 90 # CHANGE THIS\n",
        "ablation1 = \"none\"\n",
        "ablation2 = \"none\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOmlbD0TILKj"
      },
      "outputs": [],
      "source": [
        "train_model(modified_resnet, dataloader, manifold_matching_loss, sinkhorn_loss, text2img_dim_transform, num_epochs, device, all_prompt_features, validation_loader, get_encoded_labels, ablation1, ablation2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
